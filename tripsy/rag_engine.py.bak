from .brain import generate_answer
from .web_search import web_search
from .web_fetch import fetch_page_text

def retrieve_and_answer(query: str) -> str:
    # TODO: integrate Milvus retrieval here; for now we rely on live web when no RAG
    retrieved_context = ""  # placeholder: no local context

    if not retrieved_context.strip():
        hits = web_search(query, max_results=5)
        web_ctx_parts = []
        used_domains = set()
        for h in hits[:3]:  # fetch top 3 pages for depth
            url = h.get("href","")
            if not url:
                continue
            text = fetch_page_text(url)
            if not text:
                continue
            title = (h.get("title") or "").strip()
            domain = url.split("/")[2] if "://" in url else url
            used_domains.add(domain)
            snippet = text[:1500]
            web_ctx_parts.append(f"[{title}] {snippet}\n(source: {url})")
        web_ctx = "\n\n".join(web_ctx_parts)

        if web_ctx:
            # add a light sources footer to help the model cite
            web_ctx += "\n\nSources: " + ", ".join(sorted(used_domains))[:500]
            return generate_answer(query, context=web_ctx)

        # If even fetch failed, fall back to pure LLM general knowledge
        return generate_answer(query, context=None)

    # If you wire Milvus later, blend: retrieved_context + optional web_ctx
    return generate_answer(query, context=retrieved_context)
