import requests

OLLAMA_API = "http://127.0.0.1:11434/api/generate"
MODEL_NAME = "llama3.1:8b-instruct-q4_K_M"  # or mistral:7b-instruct if lighter

def generate_answer(prompt: str, context: str | None = None) -> str:
    """
    Send a prompt to Ollama and return the model's reply.
    """
    full_prompt = (
        f"Context:\n{context}\n\nUser: {prompt}\nAssistant:" if context else prompt
    )
    resp = requests.post(
        OLLAMA_API,
        json={"model": MODEL_NAME, "prompt": full_prompt, "stream": False},
        timeout=60,
    )
    resp.raise_for_status()
    data = resp.json()
    return data.get("response", "").strip()
