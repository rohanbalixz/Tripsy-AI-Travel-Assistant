import os
from fastapi import FastAPI, Request
from pydantic import BaseModel
from ollama import Client
from backend.app import rag  # our RAG helper

app = FastAPI()
ollama_client = Client()

class UserMessage(BaseModel):
    message: str

@app.get("/health")
async def health():
    return {"status": "ok"}

@app.post("/ask")
async def ask(user_message: UserMessage):
    query = user_message.message

    # ğŸ” Step 1: Retrieve relevant chunks from Milvus
    context_chunks = rag.search_chunks(query)
    context = "\n".join(context_chunks)

    # ğŸ§  Step 2: Build augmented prompt
    augmented_prompt = f"Context:\n{context}\n\nUser question: {query}\n\nAnswer:"

    # ğŸ¤– Step 3: Send to LLM
    response = ollama_client.chat(
        model=os.getenv("LLM_MODEL", "llama3.1:8b-instruct-q4_K_M"),
        messages=[{"role": "user", "content": augmented_prompt}],
    )

    return {"answer": response["message"]["content"]}
